#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IAÂ³ NeurogÃªnese AvanÃ§ada (CPU) - Patch Darwin v2
- Net2Wider: duplica 1 neurÃ´nio e redistribui pesos na prÃ³xima camada
- Net2Deeper: insere fc_mid ~ identidade (morfismo prÃ³ximo Ã  funÃ§Ã£o original)
- MixedActivation (DARTS-like): Î± treinÃ¡veis para escolher ReLU/Tanh/GELU/SiLU
- EWC/MAS para preservaÃ§Ã£o de conhecimento
- MÃ©tricas Prometheus avanÃ§adas com histogramas
ReferÃªncias:
- Network Morphism (generaliza Net2Net; inclui Net2Deeper/Net2Wider). arXiv:1603.01670
- DARTS: Differentiable Architecture Search (ICLR 2019)
- EWC: Overcoming catastrophic forgetting in neural networks. arXiv:1612.00796
"""

import os
import math
import random
from dataclasses import dataclass
from typing import Dict, Any, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    from .ia3_metrics import ensure_metrics, observe_step
except ImportError:
    ensure_metrics = lambda *args, **kwargs: None
    observe_step = lambda *args, **kwargs: None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MIXED ACTIVATION (DARTS-LIKE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MixedActivation(nn.Module):
    """
    AtivaÃ§Ã£o mista estilo DARTS: combinaÃ§Ã£o diferenciÃ¡vel de {ReLU, Tanh, GELU, SiLU}
    Pesos Î± sÃ£o treinÃ¡veis; softmax(Î±) gera mistura.
    """
    def __init__(self, init_alpha: float = 1.0):
        super().__init__()
        self.ops = nn.ModuleDict({
            "relu": nn.ReLU(),
            "tanh": nn.Tanh(),
            "gelu": nn.GELU(),
            "silu": nn.SiLU()
        })
        self.names = list(self.ops.keys())
        self.alpha = nn.Parameter(torch.ones(len(self.ops)) * init_alpha)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        weights = torch.softmax(self.alpha, dim=0)
        result = 0.0
        for i, name in enumerate(self.names):
            result = result + weights[i] * self.ops[name](x)
        return result

    def get_dominant_activation(self) -> Tuple[str, float]:
        """Retorna ativaÃ§Ã£o dominante e seu peso"""
        weights = torch.softmax(self.alpha, dim=0).detach()
        idx = int(torch.argmax(weights).item())
        return self.names[idx], float(weights[idx].item())

    def entropy(self) -> float:
        """Entropia da distribuiÃ§Ã£o de ativaÃ§Ãµes"""
        weights = torch.softmax(self.alpha, dim=0)
        eps = 1e-9
        return float((-weights * (weights + eps).log()).sum().item())

    def reset_to_activation(self, activation_name: str):
        """Reset Î± para favorecer uma ativaÃ§Ã£o especÃ­fica"""
        if activation_name in self.names:
            idx = self.names.index(activation_name)
            with torch.no_grad():
                self.alpha.zero_()
                self.alpha[idx] = 3.0  # Favorece esta ativaÃ§Ã£o

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DYNAMIC MLP COM NET2WIDER + NET2DEEPER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class DynamicMLP(nn.Module):
    """
    MLP dinÃ¢mico que pode crescer preservando funÃ§Ã£o:
      - Net2Wider: hidden_dim += 1 (duplica neurÃ´nio e redistribui pesos)
      - Net2Deeper: insere camada fc_mid ~ identidade
    Fluxo:
      x -> fc1 -> mixed_act -> (fc_mid -> mixed_act_mid)? -> fc2
    """
    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int, mixed_act_init: float = 1.0):
        super().__init__()
        self.in_dim = in_dim
        self.hidden_dim = hidden_dim
        self.out_dim = out_dim
        self.has_mid = False
        
        # Camadas principais
        self.fc1 = nn.Linear(in_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, out_dim)
        
        # AtivaÃ§Ãµes mistas treinÃ¡veis
        self.mixed_act = MixedActivation(init_alpha=mixed_act_init)
        self.mixed_act_mid = None  # Criada quando Net2Deeper
        
        # InicializaÃ§Ã£o Kaiming
        self._reset_parameters()
        
        # HistÃ³rico para EWC/MAS
        self.previous_weights = {}
        self.fisher_information = {}
        self.gradient_importance = {}

    def _reset_parameters(self):
        """InicializaÃ§Ã£o cuidadosa dos parÃ¢metros"""
        nn.init.kaiming_uniform_(self.fc1.weight, a=math.sqrt(5))
        nn.init.zeros_(self.fc1.bias)
        nn.init.kaiming_uniform_(self.fc2.weight, a=math.sqrt(5))
        nn.init.zeros_(self.fc2.bias)
        
        if hasattr(self, 'fc_mid') and self.fc_mid is not None:
            nn.init.eye_(self.fc_mid.weight)
            nn.init.zeros_(self.fc_mid.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass com ativaÃ§Ãµes mistas"""
        h = self.mixed_act(self.fc1(x))
        
        if self.has_mid and self.fc_mid is not None:
            if self.mixed_act_mid is None:
                self.mixed_act_mid = MixedActivation()
            h = self.mixed_act_mid(self.fc_mid(h))
        
        y = self.fc2(h)
        return y

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # NET2WIDER: Adicionar neurÃ´nio preservando funÃ§Ã£o
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    @torch.no_grad()
    def add_neuron_net2wider(self) -> int:
        """
        Net2Wider: aumenta hidden_dim + 1.
        EstratÃ©gia: duplicar um neurÃ´nio j em fc1 e dividir seu fluxo na fc2.
        Retorna: Ã­ndice do neurÃ´nio adicionado
        """
        device = self.fc1.weight.device
        dtype = self.fc1.weight.dtype
        
        # Escolher neurÃ´nio para duplicar (aleatÃ³rio ou o de maior norma)
        weight_norms = torch.norm(self.fc1.weight, dim=1)
        j = random.randrange(self.hidden_dim)  # ou torch.argmax(weight_norms).item()

        # Expandir fc1: (hidden_dim, in_dim) -> (hidden_dim+1, in_dim)
        W1_old, b1_old = self.fc1.weight.data, self.fc1.bias.data
        W1_new = torch.zeros((self.hidden_dim + 1, self.in_dim), device=device, dtype=dtype)
        b1_new = torch.zeros((self.hidden_dim + 1,), device=device, dtype=dtype)
        
        # Copiar neurÃ´nios existentes
        W1_new[:self.hidden_dim] = W1_old
        b1_new[:self.hidden_dim] = b1_old
        
        # Novo neurÃ´nio: cÃ³pia do j-Ã©simo + ruÃ­do pequeno
        W1_new[self.hidden_dim] = W1_old[j] + 1e-3 * torch.randn_like(W1_old[j])
        b1_new[self.hidden_dim] = b1_old[j] + 1e-3 * torch.randn(1).item()

        # Expandir fc2: (out_dim, hidden_dim) -> (out_dim, hidden_dim+1)
        W2_old, b2_old = self.fc2.weight.data, self.fc2.bias.data
        W2_new = torch.zeros((self.out_dim, self.hidden_dim + 1), device=device, dtype=dtype)
        
        # Copiar conexÃµes existentes
        W2_new[:, :self.hidden_dim] = W2_old
        
        # Dividir o fluxo: neurÃ´nio j e novo neurÃ´nio dividem responsabilidade
        W2_new[:, j] = W2_old[:, j] * 0.5  # Reduzir original
        W2_new[:, self.hidden_dim] = W2_old[:, j] * 0.5  # Novo com mesma responsabilidade
        
        b2_new = b2_old.clone()

        # Aplicar mudanÃ§as
        self.hidden_dim += 1
        self.fc1 = nn.Linear(self.in_dim, self.hidden_dim).to(device=device, dtype=dtype)
        self.fc2 = nn.Linear(self.hidden_dim, self.out_dim).to(device=device, dtype=dtype)
        
        self.fc1.weight.data.copy_(W1_new)
        self.fc1.bias.data.copy_(b1_new)
        self.fc2.weight.data.copy_(W2_new)
        self.fc2.bias.data.copy_(b2_new)

        # Expandir fc_mid se existir (preservar identidade expandida)
        if self.has_mid and hasattr(self, 'fc_mid'):
            Wm_old, bm_old = self.fc_mid.weight.data, self.fc_mid.bias.data
            Wm_new = torch.eye(self.hidden_dim, device=device, dtype=dtype)
            bm_new = torch.zeros((self.hidden_dim,), device=device, dtype=dtype)
            
            # Copiar bloco antigo
            Wm_new[:self.hidden_dim-1, :self.hidden_dim-1] = Wm_old
            bm_new[:self.hidden_dim-1] = bm_old
            
            self.fc_mid = nn.Linear(self.hidden_dim, self.hidden_dim).to(device=device, dtype=dtype)
            self.fc_mid.weight.data.copy_(Wm_new)
            self.fc_mid.bias.data.copy_(bm_new)

        return self.hidden_dim - 1  # Ãndice do neurÃ´nio adicionado

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # NET2DEEPER: Inserir camada preservando funÃ§Ã£o
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    @torch.no_grad()
    def net2deeper_insert(self) -> bool:
        """
        Net2Deeper: insere camada h->h iniciada como ~identidade (preserva funÃ§Ã£o).
        (Para ReLU, identidade preserva; para outras ativaÃ§Ãµes, Ã© aproximaÃ§Ã£o prÃ³xima.)
        """
        if self.has_mid:
            return False  # JÃ¡ tem camada intermediÃ¡ria
        
        device = self.fc1.weight.device
        dtype = self.fc1.weight.dtype
        
        # Criar nova camada intermediÃ¡ria como identidade
        self.fc_mid = nn.Linear(self.hidden_dim, self.hidden_dim).to(device=device, dtype=dtype)
        
        with torch.no_grad():
            # Inicializar como matriz identidade para preservar funÃ§Ã£o
            self.fc_mid.weight.data.copy_(torch.eye(self.hidden_dim, device=device, dtype=dtype))
            self.fc_mid.bias.data.zero_()
        
        # Criar ativaÃ§Ã£o mista para camada intermediÃ¡ria
        self.mixed_act_mid = MixedActivation()
        
        self.has_mid = True
        print(f"ğŸ¯ Net2Deeper: Camada intermediÃ¡ria inserida (identidade preservada)")
        return True

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # REMOÃ‡ÃƒO DE NEURÃ”NIOS (EQUAÃ‡ÃƒO DA MORTE)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    @torch.no_grad()
    def remove_worst_neuron(self) -> Tuple[bool, int]:
        """Remove neurÃ´nio com menor contribuiÃ§Ã£o (norma dos pesos de saÃ­da)"""
        if self.hidden_dim <= 1:
            return False, -1
        
        # Calcular contribuiÃ§Ã£o de cada neurÃ´nio (norma das conexÃµes de saÃ­da)
        contributions = torch.norm(self.fc2.weight, dim=0)  # [hidden_dim]
        worst_idx = torch.argmin(contributions).item()
        
        return self._remove_neuron_at_index(worst_idx), worst_idx
    
    @torch.no_grad()
    def _remove_neuron_at_index(self, idx: int) -> bool:
        """Remove neurÃ´nio em Ã­ndice especÃ­fico"""
        if self.hidden_dim <= 1 or idx >= self.hidden_dim:
            return False
        
        device = self.fc1.weight.device
        dtype = self.fc1.weight.dtype
        
        # Criar mÃ¡scara (manter todos exceto idx)
        keep_mask = torch.ones(self.hidden_dim, dtype=torch.bool)
        keep_mask[idx] = False
        
        # Reduzir fc1: manter linhas != idx
        W1_new = self.fc1.weight.data[keep_mask, :]
        b1_new = self.fc1.bias.data[keep_mask]
        
        # Reduzir fc2: manter colunas != idx
        W2_new = self.fc2.weight.data[:, keep_mask]
        b2_new = self.fc2.bias.data.clone()
        
        # Aplicar mudanÃ§as
        self.hidden_dim -= 1
        self.fc1 = nn.Linear(self.in_dim, self.hidden_dim).to(device=device, dtype=dtype)
        self.fc2 = nn.Linear(self.hidden_dim, self.out_dim).to(device=device, dtype=dtype)
        
        self.fc1.weight.data.copy_(W1_new)
        self.fc1.bias.data.copy_(b1_new)
        self.fc2.weight.data.copy_(W2_new)
        self.fc2.bias.data.copy_(b2_new)
        
        # Reduzir fc_mid se existir
        if self.has_mid and hasattr(self, 'fc_mid'):
            Wm_new = self.fc_mid.weight.data[keep_mask][:, keep_mask]
            bm_new = self.fc_mid.bias.data[keep_mask]
            
            self.fc_mid = nn.Linear(self.hidden_dim, self.hidden_dim).to(device=device, dtype=dtype)
            self.fc_mid.weight.data.copy_(Wm_new)
            self.fc_mid.bias.data.copy_(bm_new)
        
        return True

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TREINO COM DUPLO OTIMIZADOR (PESOS + ALPHA)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def train_short(self, X: torch.Tensor, y: torch.Tensor,
                    steps: int = 64, lr: float = 1e-3, alpha_lr: float = 5e-4,
                    alpha_step_every: int = 4, use_ewc: bool = True) -> Dict[str, Any]:
        """
        Treina pesos e Î± (MixedActivation) com regularizaÃ§Ã£o EWC/MAS.
        Dois otimizadores: um para pesos normais, outro para Î± arquiteturais.
        """
        self.train()
        device = next(self.parameters()).device
        X = X.to(device)
        y = y.to(device)

        # Separar parÃ¢metros: pesos vs alpha
        weight_params = []
        alpha_params = []
        
        for name, param in self.named_parameters():
            if "alpha" in name:
                alpha_params.append(param)
            else:
                weight_params.append(param)

        # Otimizadores separados
        opt_weights = torch.optim.Adam(weight_params, lr=lr)
        opt_alpha = torch.optim.Adam(alpha_params, lr=alpha_lr) if alpha_params else None

        last_loss = None
        grad_norm = 0.0
        
        for step in range(steps):
            # Treino dos pesos
            opt_weights.zero_grad()
            if opt_alpha:
                opt_alpha.zero_grad()
            
            y_pred = self.forward(X)
            loss = F.mse_loss(y_pred, y)
            
            # Adicionar regularizaÃ§Ã£o EWC se disponÃ­vel
            if use_ewc and hasattr(self, '_ewc_penalty'):
                loss = loss + self._ewc_penalty()
            
            loss.backward()

            # Calcular norma do gradiente (apenas pesos, nÃ£o Î±)
            total_grad_norm = 0.0
            for param in weight_params:
                if param.grad is not None:
                    total_grad_norm += param.grad.detach().pow(2).sum().item()
            grad_norm = math.sqrt(total_grad_norm + 1e-12)

            # Gradient clipping
            if weight_params:
                torch.nn.utils.clip_grad_norm_(weight_params, max_norm=1.0)
            if alpha_params:
                torch.nn.utils.clip_grad_norm_(alpha_params, max_norm=0.5)

            # Update pesos
            opt_weights.step()
            
            # Update Î± com frequÃªncia menor
            if opt_alpha and step % alpha_step_every == 0:
                opt_alpha.step()

            last_loss = float(loss.detach().item())

            # Observar mÃ©tricas
            try:
                act_choice, act_weight = self.mixed_act.get_dominant_activation()
                observe_step(
                    loss=last_loss,
                    grad_norm=grad_norm,
                    weight_norm=self.weight_l2_norm(),
                    act_entropy=self.mixed_act.entropy(),
                    act_choice=act_choice,
                    act_weight=act_weight
                )
            except Exception:
                pass

        return {
            "loss": last_loss,
            "grad_norm": grad_norm,
            "act_choice": self.mixed_act.get_dominant_activation(),
            "act_entropy": self.mixed_act.entropy(),
            "weight_norm": self.weight_l2_norm()
        }

    @torch.no_grad()
    def weight_l2_norm(self) -> float:
        """Calcula norma L2 total dos pesos (excluindo Î±)"""
        total = 0.0
        for name, param in self.named_parameters():
            if param.data is None or "alpha" in name:
                continue
            total += float((param.data ** 2).sum().item())
        return math.sqrt(total + 1e-12)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # EWC/MAS PARA PRESERVAÃ‡ÃƒO DE CONHECIMENTO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def consolidate_knowledge(self, X: torch.Tensor, y: torch.Tensor):
        """Consolida conhecimento importante usando aproximaÃ§Ã£o EWC"""
        self.eval()
        
        # Calcular importÃ¢ncia dos parÃ¢metros (Fisher Information aproximada)
        self.zero_grad()
        y_pred = self.forward(X)
        loss = F.mse_loss(y_pred, y)
        loss.backward()
        
        # Salvar importÃ¢ncia (gradiente^2) e valores atuais
        for name, param in self.named_parameters():
            if param.grad is not None and "alpha" not in name:
                # Fisher diagonal aproximado
                self.fisher_information[name] = param.grad.detach() ** 2
                self.previous_weights[name] = param.detach().clone()

    def _ewc_penalty(self) -> torch.Tensor:
        """Calcula penalidade EWC para preservar conhecimento importante"""
        penalty = 0.0
        ewc_lambda = 2.0
        
        for name, param in self.named_parameters():
            if name in self.fisher_information and name in self.previous_weights:
                fisher = self.fisher_information[name]
                prev_weights = self.previous_weights[name]
                penalty += (fisher * (param - prev_weights) ** 2).sum()
        
        return ewc_lambda * penalty

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GERAÃ‡ÃƒO DE DADOS SINTÃ‰TICOS AVANÃ‡ADOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def make_synthetic_identity(n_samples=4096, n_features=16, n_targets=16, device="cpu"):
    """Dados sintÃ©ticos para auto-supervisÃ£o (identidade com ruÃ­do)"""
    X = torch.randn(n_samples, n_features, device=device)
    
    # Target: vÃ¡rias transformaÃ§Ãµes para testar capacidades diferentes
    Y = torch.zeros(n_samples, n_targets, device=device)
    
    # Identidade bÃ¡sica
    Y[:, :min(n_features, n_targets)] = X[:, :min(n_features, n_targets)]
    
    # TransformaÃ§Ãµes nÃ£o-lineares para testar adaptaÃ§Ã£o
    if n_targets > n_features:
        Y[:, n_features:n_features+2] = torch.stack([
            torch.sin(X[:, 0] * 3.14159),
            torch.cos(X[:, 1] * 3.14159)
        ], dim=1)
    
    # Adicionar ruÃ­do controlado
    Y = Y + 0.01 * torch.randn_like(Y)
    
    return X, Y

def make_ood_batch(base_X: torch.Tensor, base_y: torch.Tensor, 
                   shift_factor: float = 0.1) -> Tuple[torch.Tensor, torch.Tensor]:
    """Cria batch Out-of-Distribution para teste de adaptaÃ§Ã£o"""
    device = base_X.device
    
    # Shift na distribuiÃ§Ã£o
    X_ood = base_X + shift_factor * torch.randn_like(base_X)
    
    # RotaÃ§Ã£o leve no espaÃ§o de features
    rotation_angle = shift_factor * 0.1
    cos_theta = math.cos(rotation_angle)
    sin_theta = math.sin(rotation_angle)
    
    if base_X.size(1) >= 2:
        # Aplicar rotaÃ§Ã£o nas duas primeiras dimensÃµes
        X_ood_rotated = X_ood.clone()
        X_ood_rotated[:, 0] = cos_theta * X_ood[:, 0] - sin_theta * X_ood[:, 1] 
        X_ood_rotated[:, 1] = sin_theta * X_ood[:, 0] + cos_theta * X_ood[:, 1]
        X_ood = X_ood_rotated
    
    # Manter same target structure
    y_ood = base_y + shift_factor * 0.05 * torch.randn_like(base_y)
    
    return X_ood, y_ood

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AVALIADOR IAÂ³ AVANÃ‡ADO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class IA3NeuronReport:
    adaptive_gain: float
    alpha_shift: float
    improved: bool
    structure_grew: bool
    passes: bool
    consciousness_score: float
    details: Dict[str, Any]

class IA3NeuronAgent:
    """
    Agente 'neurÃ´nio' IAÂ³-like avanÃ§ado:
    - Prova adaptatividade com dados OOD
    - AutoevoluÃ§Ã£o via Î± treinÃ¡veis (DARTS-like)
    - Autoarquitetura via Net2Wider/Net2Deeper
    - Autodidaxia via auto-supervisÃ£o
    - Consciousness proxy baseado em complexidade e adaptabilidade
    """
    def __init__(self, in_dim=16, hidden_dim=8, out_dim=16, metrics_port: int = 9091):
        self.model = DynamicMLP(in_dim, hidden_dim, out_dim, mixed_act_init=1.0)
        self.generation = 0
        self.total_adaptations = 0
        self.survival_rounds = 0
        
        ensure_metrics(port=metrics_port)
        print(f"ğŸ§¬ IAÂ³ Neuron Agent inicializado")
        print(f"   DimensÃµes: {in_dim} â†’ {hidden_dim} â†’ {out_dim}")
        print(f"   MÃ©tricas: porta {metrics_port}")

    def round(self, seed: int = 42, grow_policy: str = "wider_then_deeper") -> IA3NeuronReport:
        """Executa uma rodada completa de avaliaÃ§Ã£o IAÂ³"""
        self.generation += 1
        
        print(f"\nğŸ”¬ AVALIAÃ‡ÃƒO IAÂ³ - GeraÃ§Ã£o {self.generation}")
        print(f"   NeurÃ´nios atuais: {self.model.hidden_dim}")
        print(f"   Tem camada mid: {self.model.has_mid}")
        
        # Gerar dados com seed fixo para reprodutibilidade
        torch.manual_seed(seed)
        X, Y = make_synthetic_identity(
            n_samples=256, 
            n_features=self.model.in_dim, 
            n_targets=self.model.out_dim, 
            device=next(self.model.parameters()).device
        )

        # Medir performance baseline
        with torch.no_grad():
            baseline_pred = self.model(X)
            baseline_loss = F.mse_loss(baseline_pred, Y).item()
            baseline_alpha = self.model.mixed_act.alpha.detach().clone()

        print(f"   ğŸ“Š Loss baseline: {baseline_loss:.6f}")

        # Treino curto com duplo otimizador
        print(f"   ğŸ¯ Treinando com otimizadores duplos...")
        train_stats = self.model.train_short(
            X, Y, 
            steps=64, 
            lr=1e-3, 
            alpha_lr=5e-4,
            alpha_step_every=4,
            use_ewc=True
        )

        # Teste de adaptaÃ§Ã£o OOD
        print(f"   ğŸŒŠ Testando adaptaÃ§Ã£o OOD...")
        X_ood, Y_ood = make_ood_batch(X, Y, shift_factor=0.15)
        
        with torch.no_grad():
            ood_pred = self.model(X_ood)
            ood_loss = F.mse_loss(ood_pred, Y_ood).item()
            post_alpha = self.model.mixed_act.alpha.detach().clone()

        # Crescimento estrutural
        print(f"   ğŸ—ï¸ Aplicando crescimento estrutural ({grow_policy})...")
        structure_grew = False
        
        if grow_policy == "wider_then_deeper":
            if random.random() < 0.7:  # 70% chance: Net2Wider
                new_idx = self.model.add_neuron_net2wider()
                structure_grew = True
                print(f"      âœ… Net2Wider: +1 neurÃ´nio (#{new_idx})")
            elif not self.model.has_mid:
                if self.model.net2deeper_insert():
                    structure_grew = True
                    print(f"      âœ… Net2Deeper: +1 camada")
        elif grow_policy == "deeper_first" and not self.model.has_mid:
            if self.model.net2deeper_insert():
                structure_grew = True
                print(f"      âœ… Net2Deeper: +1 camada")
        else:
            new_idx = self.model.add_neuron_net2wider()
            structure_grew = True
            print(f"      âœ… Net2Wider: +1 neurÃ´nio (#{new_idx})")

        # Consolidar conhecimento apÃ³s crescimento
        if structure_grew:
            self.model.consolidate_knowledge(X, Y)

        # Medir melhorias
        adaptive_gain = max(0.0, baseline_loss - ood_loss)
        alpha_shift = float((post_alpha - baseline_alpha).abs().sum().item())
        improved = ood_loss < baseline_loss * 0.95  # 5% de tolerÃ¢ncia

        # Calcular consciousness proxy
        complexity = (
            0.3 * self.model.hidden_dim / 64 +  # Complexidade estrutural
            0.3 * (1 - self.model.mixed_act.entropy() / math.log(4)) +  # EspecializaÃ§Ã£o
            0.2 * min(1.0, adaptive_gain * 100) +  # Adaptabilidade
            0.2 * min(1.0, self.survival_rounds * 0.1)  # MemÃ³ria temporal
        )
        consciousness_score = min(1.0, max(0.0, complexity))

        # CritÃ©rios IAÂ³ rigorosos
        passes = (
            adaptive_gain > 1e-3 and      # Adaptativo
            alpha_shift > 1e-3 and        # Autoevolutivo (Î± mudou)
            improved and                  # Autodidata (melhorou)
            structure_grew and            # Autoconstrutivo
            train_stats["grad_norm"] > 1e-6 and  # Autorecursivo
            consciousness_score > 0.3     # Consciousness mÃ­nima
        )

        if passes:
            self.survival_rounds += 1
            self.total_adaptations += 1
            print(f"   âœ… NEURÃ”NIO APROVADO! Score consciÃªncia: {consciousness_score:.3f}")
        else:
            self.survival_rounds = 0
            print(f"   âŒ NEURÃ”NIO REPROVADO! Falhas detectadas.")

        return IA3NeuronReport(
            adaptive_gain=adaptive_gain,
            alpha_shift=alpha_shift,
            improved=improved,
            structure_grew=structure_grew,
            passes=passes,
            consciousness_score=consciousness_score,
            details={
                "baseline_loss": baseline_loss,
                "ood_loss": ood_loss,
                "train_stats": train_stats,
                "hidden_dim": self.model.hidden_dim,
                "has_mid": self.model.has_mid,
                "generation": self.generation,
                "survival_rounds": self.survival_rounds,
                "total_adaptations": self.total_adaptations
            }
        )

    def get_model_summary(self) -> Dict[str, Any]:
        """Retorna resumo do estado do modelo"""
        dominant_act, act_weight = self.model.mixed_act.get_dominant_activation()
        
        return {
            "architecture": {
                "input_dim": self.model.in_dim,
                "hidden_dim": self.model.hidden_dim,
                "output_dim": self.model.out_dim,
                "has_middle_layer": self.model.has_mid
            },
            "activation": {
                "dominant": dominant_act,
                "weight": act_weight,
                "entropy": self.model.mixed_act.entropy()
            },
            "parameters": {
                "total": sum(p.numel() for p in self.model.parameters()),
                "weight_norm": self.model.weight_l2_norm()
            },
            "evolution": {
                "generation": self.generation,
                "survival_rounds": self.survival_rounds,
                "total_adaptations": self.total_adaptations
            }
        }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INTERFACE PARA TESTES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_demo(rounds: int = 3, metrics_port: int = 9091):
    """DemonstraÃ§Ã£o do sistema IAÂ³ avanÃ§ado"""
    print("ğŸ§¬ DEMO SISTEMA IAÂ³ NEUROGENESIS AVANÃ‡ADO")
    print("="*60)
    
    agent = IA3NeuronAgent(
        in_dim=16, 
        hidden_dim=4, 
        out_dim=16,
        metrics_port=metrics_port
    )
    
    for round_num in range(1, rounds + 1):
        print(f"\nğŸ”„ ROUND {round_num}/{rounds}")
        
        # Executar rodada IAÂ³
        report = agent.round(
            seed=42 + round_num,
            grow_policy="wider_then_deeper"
        )
        
        # Mostrar resultados
        print(f"ğŸ“Š RESULTADOS:")
        print(f"   AdaptaÃ§Ã£o OOD: {report.adaptive_gain:.6f}")
        print(f"   MudanÃ§a Î±: {report.alpha_shift:.6f}")
        print(f"   Melhorou: {report.improved}")
        print(f"   Cresceu: {report.structure_grew}")
        print(f"   APROVADO: {'âœ…' if report.passes else 'âŒ'}")
        print(f"   ConsciÃªncia: {report.consciousness_score:.3f}")
        
        # Mostrar resumo do modelo
        summary = agent.get_model_summary()
        arch = summary["architecture"]
        act = summary["activation"]
        print(f"   Arquitetura: {arch['input_dim']}â†’{arch['hidden_dim']}â†’{arch['output_dim']} (mid: {arch['has_middle_layer']})")
        print(f"   AtivaÃ§Ã£o dominante: {act['dominant']} ({act['weight']:.3f})")
        
        # Aplicar EquaÃ§Ã£o da Morte se necessÃ¡rio
        if not report.passes:
            print(f"   â˜ ï¸ APLICANDO EQUAÃ‡ÃƒO DA MORTE...")
            removed, worst_idx = agent.model.remove_worst_neuron()
            if removed:
                print(f"      ğŸ’€ NeurÃ´nio #{worst_idx} removido")
            
            # Adicionar neurÃ´nio substituto
            new_idx = agent.model.add_neuron_net2wider()
            print(f"      ğŸ£ NeurÃ´nio substituto #{new_idx} criado")
        
        time.sleep(1)  # Pausa para observabilidade

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--demo":
        rounds = int(sys.argv[2]) if len(sys.argv) > 2 else 3
        port = int(sys.argv[3]) if len(sys.argv) > 3 else 9091
        run_demo(rounds, port)
    else:
        print("Uso: python ia3_neurogenesis.py --demo [rounds] [metrics_port]")
        print("Exemplo: python ia3_neurogenesis.py --demo 5 9091")