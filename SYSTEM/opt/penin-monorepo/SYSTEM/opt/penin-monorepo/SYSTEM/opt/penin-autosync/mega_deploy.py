#!/usr/bin/env python3
"""
MEGA DEPLOY SYSTEM - Deploy COMPLETO e ORGANIZADO de TUDO
Sistema para fazer deploy de TODO o conte√∫do do computador de forma organizada
"""

import os
import sys
import json
import yaml
import shutil
import subprocess
import hashlib
from pathlib import Path
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('MEGA-DEPLOY')

# Configura√ß√£o do MEGA DEPLOY
REPO_PATH = '/opt/penin-monorepo'
GITHUB_TOKEN = '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'
GITHUB_USER = 'danielgonzagat'
GITHUB_REPO = 'penin-monorepo'

# CONGLOMERADOS - Organiza√ß√£o l√≥gica de TODO o sistema
CONGLOMERATES = {
    "üß†_NEURAL_SYSTEMS": {
        "description": "Sistemas Neurais e IA Principal",
        "paths": [
            "/root/neural_farm_prod",
            "/root/neural_farm_backup_*",
            "/root/complete_brain",
            "/root/IA3_SUPREME",
            "/root/agi_penin_transcendent",
            "/root/ia3_infinite_backup_*",
            "/root/.falcon_brain",
            "/root/.falcon_identity",
            "/root/.falcon_q_unified",
            "/opt/et_ultimate"
        ]
    },
    "ü§ñ_AI_FRAMEWORKS": {
        "description": "Frameworks e Bibliotecas de IA",
        "paths": [
            "/root/litellm",
            "/root/langgraph",
            "/root/llama_index",
            "/root/llama.cpp",
            "/root/autokeras",
            "/root/dspy",
            "/root/trl",
            "/root/clip",
            "/root/transformers*",
            "/root/fusion-agi"
        ]
    },
    "üë•_AGENT_SYSTEMS": {
        "description": "Sistemas de Agentes e Multi-Agentes",
        "paths": [
            "/root/agents_framework",
            "/root/agent-squad",
            "/root/hivemind",
            "/root/openhands",
            "/root/smol-dev",
            "/root/agi-alpha-real",
            "/tmp/agi-alpha",
            "/root/.agent_creation_throttled"
        ]
    },
    "üîß_DEVELOPMENT": {
        "description": "Ambientes de Desenvolvimento e Projetos",
        "paths": [
            "/root/playwright",
            "/root/home_assistant",
            "/root/third_party",
            "/root/projetos",
            "/root/smol_dev_runs",
            "/root/.cursor*",
            "/root/.config",
            "/root/.aws"
        ]
    },
    "üìä_DATA_MODELS": {
        "description": "Dados, Modelos e Checkpoints",
        "paths": [
            "/root/data",
            "/root/uploads",
            "/root/unified_enhanced_data",
            "/root/teis_checkpoints",
            "/root/teis_v2_out_prod",
            "/root/generation_*.pt",
            "/root/.data",
            "/root/.cache"
        ]
    },
    "üîå_INTEGRATIONS": {
        "description": "Integra√ß√µes e APIs",
        "paths": [
            "/root/integrations",
            "/root/.amazonq",
            "/root/swarm_simulation*",
            "/root/o1-eng*",
            "/root/ai_dialogue_memory"
        ]
    },
    "üìù_LOGS_METRICS": {
        "description": "Logs, M√©tricas e Monitoramento",
        "paths": [
            "/root/*.log",
            "/root/oppenheimer_maestro.log",
            "/root/unified.log",
            "/root/ia3_atomic_bomb.log",
            "/var/log",
            "/opt/penin-autosync/logs"
        ]
    },
    "‚öôÔ∏è_SYSTEM_CONFIG": {
        "description": "Configura√ß√µes do Sistema",
        "paths": [
            "/root/.bashrc*",
            "/root/.env*",
            "/root/.git*",
            "/root/.docker*",
            "/root/.ssh",
            "/etc/systemd/system/*.service",
            "/usr/local/bin/*"
        ]
    },
    "üåê_WEB_SERVICES": {
        "description": "Servi√ßos Web e APIs",
        "paths": [
            "/var/www",
            "/etc/nginx",
            "/etc/apache2",
            "/root/*server*",
            "/root/*api*"
        ]
    },
    "üóÑÔ∏è_DATABASES": {
        "description": "Bancos de Dados e Armazenamento",
        "paths": [
            "/var/lib/mysql",
            "/var/lib/postgresql",
            "/root/*.db",
            "/root/*.sqlite*"
        ]
    },
    "üîí_SECURITY": {
        "description": "Seguran√ßa e Credenciais",
        "paths": [
            "/root/.convergent_systems",
            "/root/.active_knowledge",
            "/root/creds",
            "/root/secrets"
        ]
    },
    "üì¶_PACKAGES": {
        "description": "Pacotes e Depend√™ncias",
        "paths": [
            "/root/aider-env",
            "/root/.virtualenvs",
            "/root/.pip",
            "/root/.npm",
            "/usr/local/lib/python*"
        ]
    },
    "üéØ_PENIN_CORE": {
        "description": "Sistema PENIN Core",
        "paths": [
            "/opt/penin-autosync",
            "/opt/penin-monorepo",
            "/opt/penin_omega",
            "/opt/ml"
        ]
    },
    "üìö_DOCUMENTATION": {
        "description": "Documenta√ß√£o e Wikis",
        "paths": [
            "/root/*.md",
            "/root/docs",
            "/root/README*",
            "/root/wiki"
        ]
    },
    "üöÄ_DEPLOYMENT": {
        "description": "Deploy e CI/CD",
        "paths": [
            "/root/.github",
            "/root/Dockerfile*",
            "/root/docker-compose*",
            "/root/kubernetes",
            "/root/.gitlab*"
        ]
    }
}

class MegaDeploySystem:
    def __init__(self):
        self.repo_path = Path(REPO_PATH)
        self.stats = {
            'total_files': 0,
            'total_size': 0,
            'conglomerates': {},
            'errors': []
        }
        
    def prepare_repository(self):
        """Prepara o reposit√≥rio com estrutura organizada"""
        logger.info("üöÄ Preparando reposit√≥rio para MEGA DEPLOY...")
        
        # Criar estrutura de conglomerados
        for conglomerate_name in CONGLOMERATES.keys():
            conglomerate_path = self.repo_path / conglomerate_name
            conglomerate_path.mkdir(parents=True, exist_ok=True)
            
            # Criar README para cada conglomerado
            readme_content = f"""# {conglomerate_name.replace('_', ' ')}

## {CONGLOMERATES[conglomerate_name]['description']}

### üìÅ Conte√∫do

Este conglomerado cont√©m:
"""
            for path in CONGLOMERATES[conglomerate_name]['paths']:
                readme_content += f"- `{path}`\n"
                
            readme_path = conglomerate_path / "README.md"
            readme_path.write_text(readme_content)
            
    def scan_and_organize(self):
        """Escaneia TODO o sistema e organiza nos conglomerados"""
        logger.info("üîç Escaneando TODO o sistema...")
        
        for conglomerate_name, config in CONGLOMERATES.items():
            logger.info(f"\nüì¶ Processando {conglomerate_name}...")
            conglomerate_path = self.repo_path / conglomerate_name
            
            for path_pattern in config['paths']:
                # Expandir wildcards
                if '*' in path_pattern:
                    import glob
                    paths = glob.glob(path_pattern)
                else:
                    paths = [path_pattern] if os.path.exists(path_pattern) else []
                    
                for path in paths:
                    if os.path.exists(path):
                        self.process_path(path, conglomerate_path)
                        
    def process_path(self, source_path, destination_base):
        """Processa um caminho e copia para o destino organizado"""
        try:
            source = Path(source_path)
            
            # Criar nome limpo para o destino
            clean_name = source.name.replace(' ', '_').replace('/', '_')
            
            # Se for diret√≥rio com .git, √© um reposit√≥rio
            if source.is_dir() and (source / '.git').exists():
                dest_path = destination_base / f"REPO_{clean_name}"
            else:
                dest_path = destination_base / clean_name
                
            # Calcular tamanho
            if source.is_file():
                size = source.stat().st_size
                self.stats['total_size'] += size
                self.stats['total_files'] += 1
                
                # Copiar arquivo se n√£o for muito grande (limite 100MB)
                if size < 100 * 1024 * 1024:
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(source, dest_path)
                    logger.info(f"  ‚úÖ {source} ‚Üí {dest_path}")
                else:
                    # Para arquivos grandes, criar apenas um link simb√≥lico ou refer√™ncia
                    info_file = dest_path.with_suffix('.info.txt')
                    info_file.write_text(f"""# Arquivo Grande
                    
Caminho Original: {source}
Tamanho: {size / (1024*1024*1024):.2f} GB
Hash MD5: {self.calculate_hash(source)}
                    
Este arquivo √© muito grande para ser inclu√≠do diretamente.
""")
                    logger.info(f"  üìÑ {source} (muito grande, criado .info)")
                    
            elif source.is_dir():
                # Para diret√≥rios, copiar estrutura (limitando profundidade)
                self.copy_directory_smart(source, dest_path)
                
        except Exception as e:
            self.stats['errors'].append(f"{source_path}: {str(e)}")
            logger.error(f"  ‚ùå Erro em {source_path}: {e}")
            
    def copy_directory_smart(self, source_dir, dest_dir, max_depth=3, current_depth=0):
        """Copia diret√≥rio de forma inteligente com limite de profundidade"""
        if current_depth >= max_depth:
            # Criar arquivo de refer√™ncia
            info_file = dest_dir.with_suffix('.info.txt')
            info_file.write_text(f"Diret√≥rio: {source_dir}\nProfundidade m√°xima atingida.\n")
            return
            
        dest_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            for item in source_dir.iterdir():
                # Pular alguns diret√≥rios problem√°ticos
                if item.name in ['.git', '__pycache__', 'node_modules', '.venv', 'venv']:
                    continue
                    
                if item.is_file():
                    size = item.stat().st_size
                    if size < 10 * 1024 * 1024:  # Limite 10MB por arquivo
                        shutil.copy2(item, dest_dir / item.name)
                        self.stats['total_files'] += 1
                        self.stats['total_size'] += size
                elif item.is_dir():
                    self.copy_directory_smart(
                        item, 
                        dest_dir / item.name,
                        max_depth,
                        current_depth + 1
                    )
        except PermissionError:
            pass
            
    def calculate_hash(self, file_path):
        """Calcula hash MD5 de um arquivo"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except:
            return "N/A"
            
    def generate_master_index(self):
        """Gera √≠ndice mestre com navega√ß√£o completa"""
        logger.info("üìö Gerando √≠ndice mestre...")
        
        index_content = f"""# üåü PENIN MONOREPO - SISTEMA COMPLETO

> **Deploy completo de TODO o sistema em {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}**

## üìä Estat√≠sticas Gerais

- **Total de Arquivos:** {self.stats['total_files']:,}
- **Tamanho Total:** {self.stats['total_size'] / (1024*1024*1024):.2f} GB
- **Conglomerados:** {len(CONGLOMERATES)}
- **Timestamp:** {datetime.now().isoformat()}

## üóÇÔ∏è CONGLOMERADOS DO SISTEMA

"""
        
        # Adicionar cada conglomerado
        for idx, (conglomerate_name, config) in enumerate(CONGLOMERATES.items(), 1):
            emoji = conglomerate_name.split('_')[0]
            name = conglomerate_name.replace('_', ' ').strip()
            
            index_content += f"""
### {idx}. {emoji} {name}

**{config['description']}**

üìÅ [Acessar Conglomerado](./{conglomerate_name}/)

Cont√©m:
"""
            # Listar conte√∫do
            conglomerate_path = self.repo_path / conglomerate_name
            if conglomerate_path.exists():
                items = list(conglomerate_path.iterdir())[:10]  # Primeiros 10 itens
                for item in items:
                    if item.is_dir():
                        index_content += f"- üìÇ `{item.name}/`\n"
                    else:
                        index_content += f"- üìÑ `{item.name}`\n"
                        
                if len(list(conglomerate_path.iterdir())) > 10:
                    index_content += f"- ... e mais {len(list(conglomerate_path.iterdir())) - 10} itens\n"
                    
        # Adicionar navega√ß√£o
        index_content += """

---

## üîç NAVEGA√á√ÉO R√ÅPIDA

### Por Tipo de Sistema
- [üß† Sistemas Neurais](./üß†_NEURAL_SYSTEMS/)
- [ü§ñ Frameworks de IA](./ü§ñ_AI_FRAMEWORKS/)
- [üë• Sistemas de Agentes](./üë•_AGENT_SYSTEMS/)
- [üîß Desenvolvimento](./üîß_DEVELOPMENT/)

### Por Funcionalidade
- [üìä Dados e Modelos](./üìä_DATA_MODELS/)
- [üîå Integra√ß√µes](./üîå_INTEGRATIONS/)
- [üìù Logs e M√©tricas](./üìù_LOGS_METRICS/)
- [‚öôÔ∏è Configura√ß√µes](./‚öôÔ∏è_SYSTEM_CONFIG/)

### Infraestrutura
- [üåê Servi√ßos Web](./üåê_WEB_SERVICES/)
- [üóÑÔ∏è Bancos de Dados](./üóÑÔ∏è_DATABASES/)
- [üîí Seguran√ßa](./üîí_SECURITY/)
- [üì¶ Pacotes](./üì¶_PACKAGES/)

### Sistema PENIN
- [üéØ PENIN Core](./üéØ_PENIN_CORE/)
- [üìö Documenta√ß√£o](./üìö_DOCUMENTATION/)
- [üöÄ Deployment](./üöÄ_DEPLOYMENT/)

---

## üö® INFORMA√á√ïES IMPORTANTES

### Arquivos Grandes
Arquivos maiores que 100MB foram substitu√≠dos por arquivos `.info.txt` com metadados.

### Seguran√ßa
Credenciais e segredos foram movidos para o conglomerado SECURITY com prote√ß√£o adicional.

### Organiza√ß√£o
Todo o conte√∫do foi organizado em conglomerados l√≥gicos para facilitar navega√ß√£o e manuten√ß√£o.

---

## ü§ñ AGENTES CURSOR

Este reposit√≥rio est√° configurado para trabalhar com agentes Cursor que:
- Otimizam c√≥digo continuamente
- Corrigem bugs automaticamente
- Atualizam documenta√ß√£o
- Implementam melhorias
- Mant√™m seguran√ßa

---

**Sistema PENIN - Deploy Completo e Organizado**
*Sincroniza√ß√£o Bidirecional Ativa 24/7*
"""
        
        # Salvar √≠ndice
        index_path = self.repo_path / "README.md"
        index_path.write_text(index_content)
        logger.info("‚úÖ √çndice mestre gerado!")
        
    def git_operations(self):
        """Executa opera√ß√µes Git para enviar tudo"""
        logger.info("üì§ Enviando para GitHub...")
        
        os.chdir(self.repo_path)
        
        # Configurar Git
        subprocess.run(['git', 'config', 'user.name', GITHUB_USER])
        subprocess.run(['git', 'config', 'user.email', 'danielgonzagatj@gmail.com'])
        
        # Add, commit e push
        subprocess.run(['git', 'add', '-A'])
        
        commit_msg = f"MEGA DEPLOY: Sistema completo organizado - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        subprocess.run(['git', 'commit', '-m', commit_msg])
        
        # Push
        result = subprocess.run(
            ['git', 'push', 'origin', 'main', '--force'],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            logger.info("‚úÖ MEGA DEPLOY CONCLU√çDO COM SUCESSO!")
        else:
            logger.error(f"‚ùå Erro no push: {result.stderr}")
            
    def execute(self):
        """Executa o MEGA DEPLOY completo"""
        logger.info("="*60)
        logger.info("   MEGA DEPLOY SYSTEM - INICIANDO")
        logger.info("   Deploy Completo e Organizado de TUDO")
        logger.info("="*60)
        
        # 1. Preparar reposit√≥rio
        self.prepare_repository()
        
        # 2. Escanear e organizar
        self.scan_and_organize()
        
        # 3. Gerar √≠ndice mestre
        self.generate_master_index()
        
        # 4. Git operations
        self.git_operations()
        
        # 5. Relat√≥rio final
        logger.info("\n" + "="*60)
        logger.info("   RELAT√ìRIO FINAL")
        logger.info("="*60)
        logger.info(f"‚úÖ Total de arquivos: {self.stats['total_files']:,}")
        logger.info(f"‚úÖ Tamanho total: {self.stats['total_size'] / (1024*1024*1024):.2f} GB")
        logger.info(f"‚úÖ Conglomerados criados: {len(CONGLOMERATES)}")
        if self.stats['errors']:
            logger.info(f"‚ö†Ô∏è Erros encontrados: {len(self.stats['errors'])}")
        logger.info(f"üåê Reposit√≥rio: https://github.com/{GITHUB_USER}/{GITHUB_REPO}")
        logger.info("="*60)

if __name__ == "__main__":
    deployer = MegaDeploySystem()
    deployer.execute()