#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os, sys, time, json, asyncio, traceback
from pathlib import Path
from typing import List
import aiohttp
from fastapi import FastAPI
from fastapi.responses import JSONResponse, PlainTextResponse
from pydantic import BaseModel

# ---- Logs/dirs ---------------------------------------------------------------
DATA_DIR = "/opt/et_fusion/data"; os.makedirs(DATA_DIR, exist_ok=True)
LOG_DIR  = "/opt/et_fusion/logs"; os.makedirs(LOG_DIR, exist_ok=True)

def log(*a):
    msg = "[router] " + " ".join(str(x) for x in a)
    print(msg, flush=True)
    try:
        with open(f"{LOG_DIR}/router.out","a",encoding="utf-8") as f:
            f.write(msg+"\n")
    except Exception:
        pass

# ---- RAG local (ETKB) -------------------------------------------------------
ROUTER_DIR = "/opt/et_fusion/router"
if ROUTER_DIR not in sys.path:
    sys.path.append(ROUTER_DIR)
import etkb  # provê etkb.search(text, k)

# ---- Config de modelo remoto -------------------------------------------------
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY","")
OPENAI_MODEL   = os.environ.get("OPENAI_MODEL","gpt-5")  # padrão: gpt-5 (via LiteLLM na 8003)
USE_RESPONSES  = os.environ.get("OPENAI_USE_RESPONSES","0") == "1"

# Professores consultados em paralelo
TEACHERS = [
  {"name":"gpt5",                "kind":"local",  "url":"http://127.0.0.1:8003/v1/chat/completions"},
  {"name":"qwen2.5-7b",          "kind":"local",  "url":"http://127.0.0.1:8002/v1/chat/completions"},
  {"name":"deepseek-r1-qwen7b",  "kind":"local",  "url":"http://127.0.0.1:8004/v1/chat/completions"},
  {"name":"llama-3.1-8b",        "kind":"local",  "url":"http://127.0.0.1:8006/v1/chat/completions"},
]# Opcional: desabilitar professores problemáticos via env
DISABLED = set(os.getenv("DISABLE_PROFS","").split(",")) if os.getenv("DISABLE_PROFS") else set()
if DISABLED:
    _orig = list(TEACHERS)
    TEACHERS = [t for t in _orig if t["name"] not in DISABLED]

# ---- Utilitários -------------------------------------------------------------
def score_answer(txt: str) -> float:
    if not txt or not txt.strip():
        return -1e6
    t = txt.lower(); s = 0.0
    if "```" in txt: s += 0.8
    if any(x in t for x in ["passo","step","1.","2.","3."]): s += 0.6
    if len(txt) > 400: s += 0.5
    if any(x in t for x in ["desculp","não posso","i cannot"]): s -= 0.6
    return s

def build_system_preamble(rag_context: str) -> str:
    rag_context = (rag_context or "")[:3000]
    return ("Você é um PROFESSOR ETΩ. Responda com base APENAS no contexto a seguir.\n"
            "Se algo não estiver no contexto, diga que não está e evite inventar.\n"
            "=== CONTEXTO ETΩ INÍCIO ===\n" + rag_context + "\n=== CONTEXTO ETΩ FIM ===")

# ---- Consultas aos professores ----------------------------------------------
async def query_local(session, url, sys_txt, msgs, temperature, max_tokens):
    payload = {
        "model": "local",
        "temperature": float(temperature),
        "max_tokens": int(max_tokens),
        "messages": [{"role":"system","content":sys_txt}] + msgs
    }
    async with session.post(url, json=payload, timeout=180) as r:
        j = await r.json()
        # tolerar payloads diferentes
        try:
            return j["choices"][0]["message"]["content"]
        except Exception:
            return json.dumps(j)[:1500]

async def query_openai(session, base, sys_txt, msgs, temperature, max_tokens):
    if not OPENAI_API_KEY:
        return ""

    # GPT-5 (se usar LiteLLM) só aceita temperature=1
    temp = 1 if OPENAI_MODEL.startswith("gpt-5") else float(temperature)

    headers = {"Authorization":f"Bearer {OPENAI_API_KEY}","Content-Type":"application/json"}
    if USE_RESPONSES:
        payload = {
            "model": OPENAI_MODEL,
            "input": [{"role":"system","content":sys_txt}] + msgs,
            "temperature": float(temp),
            "max_output_tokens": int(max_tokens)
        }
        async with session.post(f"{base}/responses", headers=headers, json=payload, timeout=180) as r:
            j = await r.json()
            return j.get("output_text","") or json.dumps(j)[:1500]
    else:
        payload = {
            "model": OPENAI_MODEL,
            "messages": [{"role":"system","content":sys_txt}] + msgs,
            "temperature": float(temp),
            "max_tokens": int(max_tokens)
        }
        async with session.post(f"{base}/chat/completions", headers=headers, json=payload, timeout=180) as r:
            j = await r.json()
            try:
                return j["choices"][0]["message"]["content"]
            except Exception:
                return json.dumps(j)[:1500]

# ---- API --------------------------------------------------------------------
class ChatRequest(BaseModel):
    model: str = "ensemble-5x"
    messages: List[dict]
    temperature: float = 0.3
    max_tokens: int = 768
    top_k_ctx: int = 6

app = FastAPI()

@app.get("/healthz")
async def healthz():
    return PlainTextResponse("ok", status_code=200)

@app.post("/v1/chat/completions")
async def chat(req: ChatRequest):
    ts = int(time.time())
    try:
        user_text = " ".join([m.get("content","") for m in req.messages if m.get("role")=="user"])[-2000:]
        rag = etkb.search(user_text, k=int(req.top_k_ctx))
        sys_txt = build_system_preamble(rag["context"])

        async with aiohttp.ClientSession() as session:
            tasks = []
            for t in TEACHERS:
                if t["kind"] == "local":
                    tasks.append(query_local(session, t["url"], sys_txt, req.messages, req.temperature, req.max_tokens))
                else:
                    tasks.append(query_openai(session, t["url"], sys_txt, req.messages, req.temperature, req.max_tokens))
            outs = await asyncio.gather(*tasks, return_exceptions=True)

        candidates = []
        for t, o in zip(TEACHERS, outs):
            if isinstance(o, Exception):
                log("ERRO professor", t["name"], f"{type(o).__name__}: {o}")
                candidates.append({"name":t["name"], "error":str(o), "score":-1e9})
            else:
                txt = (o or "").strip()
                candidates.append({"name":t["name"], "content":txt, "score":score_answer(txt)})

        best = max(candidates, key=lambda x: x["score"])
        if best["score"] <= -1e5:
            bullets = [f"- {Path(h['file']).name} (chunk {h['chunk']})" for h in rag["hits"][:6]]
            best = {"name":"fallback-rag",
                    "content":"Sem saída útil dos professores.\nFontes RAG:\n" + "\n".join(bullets),
                    "score":0.0}

        rec = {
            "ts": ts,
            "messages": req.messages,
            "candidates": candidates,
            "best_name": best["name"],
            "best_text": best.get("content",""),
            "sources": rag["hits"]
        }
        with open(f"{DATA_DIR}/distill_{ts}.json","w",encoding="utf-8") as f:
            json.dump(rec, f, ensure_ascii=False)

        return JSONResponse({
            "id": f"chatcmpl-{ts}",
            "object": "chat.completion",
            "created": ts,
            "model": req.model,
            "choices": [{"index":0, "message":{"role":"assistant","content":best.get("content","")}, "finish_reason":"stop"}],
            "sources": rag["hits"][:5]
        })
    except Exception as e:
        log("FATAL chat:", repr(e), "\n", traceback.format_exc())
        return JSONResponse({"error": str(e)}, status_code=500)

if __name__ == "__main__":
    log("Iniciando ensemble_router com RAG e 5 professores...")
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8800, log_level="info")
