"""
Testes Pr√°ticos Avan√ßados da Equa√ß√£o de Turing
Simula√ß√µes para RL, LLMs, Rob√≥tica e Descoberta Cient√≠fica
"""

import numpy as np
import matplotlib.pyplot as plt
from et_core import ETCore, ETSignals
import logging
from typing import Dict, List, Tuple
from dataclasses import dataclass
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class SimulationResults:
    """Resultados de uma simula√ß√£o"""
    domain: str
    scores: List[float]
    acceptance_rate: float
    stability_metric: float
    final_performance: float
    convergence_time: int
    guardrails_triggered: int

class DomainSimulator:
    """Simulador base para diferentes dom√≠nios"""
    
    def __init__(self, domain_name: str):
        self.domain_name = domain_name
        self.performance_history = []
        self.guardrails_count = 0
    
    def generate_signals(self, iteration: int, performance: float) -> ETSignals:
        """Gera sinais espec√≠ficos do dom√≠nio - deve ser sobrescrito"""
        raise NotImplementedError
    
    def update_performance(self, accepted: bool, score: float) -> float:
        """Atualiza performance baseado na decis√£o - deve ser sobrescrito"""
        raise NotImplementedError

class RLSimulator(DomainSimulator):
    """Simulador para Aprendizado por Refor√ßo"""
    
    def __init__(self):
        super().__init__("Reinforcement Learning")
        self.current_return = 0.5  # Retorno inicial
        self.task_difficulties = np.array([0.5, 1.0, 1.5, 2.0])  # 4 tarefas
        self.policy_params = 1000  # N√∫mero de par√¢metros
        
    def generate_signals(self, iteration: int, performance: float) -> ETSignals:
        # Simular Learning Progress baseado em melhoria de retorno
        recent_returns = self.performance_history[-10:] if len(self.performance_history) >= 10 else [0.3]
        lp_values = []
        
        for i, difficulty in enumerate(self.task_difficulties):
            # LP maior para tarefas de dificuldade apropriada
            if 0.3 <= difficulty <= 1.5:
                lp = np.random.uniform(0.6, 0.9) * performance
            else:
                lp = np.random.uniform(0.1, 0.4) * performance
            lp_values.append(lp)
        
        # Simular entropia da pol√≠tica (diminui com converg√™ncia)
        entropy = max(0.3, 1.0 - performance * 0.5 + np.random.normal(0, 0.1))
        
        # Diverg√™ncia entre pol√≠ticas (pequena se est√°vel)
        divergence = np.random.uniform(0.05, 0.2) if performance > 0.7 else np.random.uniform(0.1, 0.4)
        
        # Drift (esquecimento) - aumenta se performance cai
        drift = max(0, 0.3 - performance) + np.random.uniform(0, 0.1)
        
        # Regret baseado em falhas em tarefas can√°rio
        regret = max(0, 0.15 - performance) + np.random.uniform(0, 0.05)
        
        return ETSignals(
            learning_progress=np.array(lp_values),
            task_difficulties=self.task_difficulties,
            mdl_complexity=self.policy_params / 10000,  # Normalizado
            energy_consumption=np.random.uniform(0.1, 0.3),
            scalability_inverse=np.random.uniform(0.1, 0.3),
            policy_entropy=entropy,
            policy_divergence=divergence,
            drift_penalty=drift,
            curriculum_variance=np.var(self.task_difficulties),
            regret_rate=regret,
            embodiment_score=np.random.uniform(0.3, 0.8),  # Simula√ß√£o f√≠sica
            phi_components=np.random.uniform(-0.3, 0.3, 4)
        )
    
    def update_performance(self, accepted: bool, score: float) -> float:
        if accepted and score > 0:
            # Melhoria baseada no score
            improvement = min(0.1, score * 0.02)
            self.current_return = min(1.0, self.current_return + improvement)
        else:
            # Pequena degrada√ß√£o se rejeitado
            self.current_return = max(0.1, self.current_return - 0.01)
        
        self.performance_history.append(self.current_return)
        return self.current_return

class LLMSimulator(DomainSimulator):
    """Simulador para Large Language Models"""
    
    def __init__(self):
        super().__init__("Large Language Model")
        self.accuracy = 0.6  # Acur√°cia inicial
        self.model_size = 7e9  # 7B par√¢metros
        self.benchmark_tasks = ["coding", "reasoning", "factual", "creative"]
        
    def generate_signals(self, iteration: int, performance: float) -> ETSignals:
        # LP baseado em melhoria em benchmarks
        lp_values = []
        difficulties = []
        
        for task in self.benchmark_tasks:
            if task == "coding":
                lp = np.random.uniform(0.7, 0.9) * performance  # C√≥digo √© mensur√°vel
                diff = 1.5
            elif task == "reasoning":
                lp = np.random.uniform(0.5, 0.8) * performance
                diff = 2.0
            elif task == "factual":
                lp = np.random.uniform(0.8, 0.95) * performance  # Fatos s√£o claros
                diff = 1.0
            else:  # creative
                lp = np.random.uniform(0.3, 0.6) * performance  # Criatividade √© subjetiva
                diff = 1.8
            
            lp_values.append(lp)
            difficulties.append(diff)
        
        # Entropia baseada na diversidade de tokens
        entropy = 0.8 + np.random.normal(0, 0.1)
        
        # Diverg√™ncia pequena (fine-tuning gradual)
        divergence = np.random.uniform(0.02, 0.1)
        
        # Drift baseado em esquecimento catastr√≥fico
        drift = max(0, 0.2 - performance) + np.random.uniform(0, 0.05)
        
        # Regret baseado em regress√£o em benchmarks
        regret = max(0, 0.1 - performance * 0.8) + np.random.uniform(0, 0.03)
        
        return ETSignals(
            learning_progress=np.array(lp_values),
            task_difficulties=np.array(difficulties),
            mdl_complexity=self.model_size / 1e10,  # Normalizado
            energy_consumption=np.random.uniform(0.05, 0.15),  # Menor com fot√¥nica
            scalability_inverse=np.random.uniform(0.15, 0.25),
            policy_entropy=entropy,
            policy_divergence=divergence,
            drift_penalty=drift,
            curriculum_variance=np.var(difficulties),
            regret_rate=regret,
            embodiment_score=0.0,  # Puramente digital
            phi_components=np.random.uniform(-0.2, 0.2, 4)
        )
    
    def update_performance(self, accepted: bool, score: float) -> float:
        if accepted and score > 0:
            # Melhoria gradual em acur√°cia
            improvement = min(0.05, score * 0.01)
            self.accuracy = min(0.95, self.accuracy + improvement)
        else:
            # Pequena degrada√ß√£o
            self.accuracy = max(0.3, self.accuracy - 0.005)
        
        self.performance_history.append(self.accuracy)
        return self.accuracy

class RoboticsSimulator(DomainSimulator):
    """Simulador para Rob√≥tica"""
    
    def __init__(self):
        super().__init__("Robotics")
        self.success_rate = 0.4  # Taxa de sucesso inicial
        self.safety_violations = 0
        self.tasks = ["navigation", "manipulation", "perception", "planning"]
        
    def generate_signals(self, iteration: int, performance: float) -> ETSignals:
        # LP baseado em melhoria em tarefas f√≠sicas
        lp_values = []
        difficulties = []
        
        for task in self.tasks:
            if task == "navigation":
                lp = np.random.uniform(0.6, 0.8) * performance
                diff = 1.2
            elif task == "manipulation":
                lp = np.random.uniform(0.4, 0.7) * performance  # Mais dif√≠cil
                diff = 2.5
            elif task == "perception":
                lp = np.random.uniform(0.7, 0.9) * performance
                diff = 1.0
            else:  # planning
                lp = np.random.uniform(0.5, 0.8) * performance
                diff = 1.8
            
            lp_values.append(lp)
            difficulties.append(diff)
        
        # Entropia baseada na diversidade de a√ß√µes
        entropy = 0.7 + np.random.normal(0, 0.15)
        
        # Diverg√™ncia (mudan√ßas na pol√≠tica de controle)
        divergence = np.random.uniform(0.1, 0.3)
        
        # Drift cr√≠tico (seguran√ßa)
        drift = max(0, 0.4 - performance) + np.random.uniform(0, 0.1)
        
        # Regret baseado em falhas de seguran√ßa
        safety_factor = max(0, self.safety_violations * 0.1)
        regret = safety_factor + max(0, 0.2 - performance) + np.random.uniform(0, 0.05)
        
        # Embodiment √© CR√çTICO em rob√≥tica
        embodiment = performance * 0.8 + np.random.uniform(0.1, 0.3)
        
        return ETSignals(
            learning_progress=np.array(lp_values),
            task_difficulties=np.array(difficulties),
            mdl_complexity=np.random.uniform(0.2, 0.4),  # Controladores
            energy_consumption=np.random.uniform(0.3, 0.6),  # Motores consomem energia
            scalability_inverse=np.random.uniform(0.2, 0.4),
            policy_entropy=entropy,
            policy_divergence=divergence,
            drift_penalty=drift,
            curriculum_variance=np.var(difficulties),
            regret_rate=regret,
            embodiment_score=embodiment,
            phi_components=np.random.uniform(-0.4, 0.4, 4)
        )
    
    def update_performance(self, accepted: bool, score: float) -> float:
        if accepted and score > 0:
            # Melhoria em taxa de sucesso
            improvement = min(0.08, score * 0.015)
            self.success_rate = min(0.9, self.success_rate + improvement)
        else:
            # Degrada√ß√£o e poss√≠vel viola√ß√£o de seguran√ßa
            self.success_rate = max(0.1, self.success_rate - 0.02)
            if np.random.random() < 0.1:  # 10% chance de viola√ß√£o
                self.safety_violations += 1
        
        self.performance_history.append(self.success_rate)
        return self.success_rate

class ScientificDiscoverySimulator(DomainSimulator):
    """Simulador para Descoberta Cient√≠fica"""
    
    def __init__(self):
        super().__init__("Scientific Discovery")
        self.discovery_rate = 0.3  # Taxa de descobertas v√°lidas
        self.hypothesis_count = 0
        self.validated_discoveries = 0
        
    def generate_signals(self, iteration: int, performance: float) -> ETSignals:
        # LP baseado em hip√≥teses que levam a descobertas
        hypothesis_types = ["chemical", "biological", "physical", "computational"]
        lp_values = []
        difficulties = []
        
        for h_type in hypothesis_types:
            if h_type == "chemical":
                lp = np.random.uniform(0.5, 0.8) * performance
                diff = 1.5
            elif h_type == "biological":
                lp = np.random.uniform(0.3, 0.6) * performance  # Mais complexo
                diff = 2.2
            elif h_type == "physical":
                lp = np.random.uniform(0.6, 0.9) * performance
                diff = 1.3
            else:  # computational
                lp = np.random.uniform(0.7, 0.9) * performance
                diff = 1.0
            
            lp_values.append(lp)
            difficulties.append(diff)
        
        # Entropia baseada na diversidade de hip√≥teses
        entropy = 0.8 + np.random.normal(0, 0.1)
        
        # Diverg√™ncia (mudan√ßa no espa√ßo de hip√≥teses)
        divergence = np.random.uniform(0.05, 0.2)
        
        # Drift (perda de conhecimento validado)
        drift = max(0, 0.3 - performance) + np.random.uniform(0, 0.08)
        
        # Regret baseado em falhas de replica√ß√£o
        replication_failures = max(0, 0.15 - performance * 0.7)
        regret = replication_failures + np.random.uniform(0, 0.04)
        
        # Embodiment baseado em integra√ß√£o com laborat√≥rio
        lab_integration = performance * 0.6 + np.random.uniform(0.2, 0.5)
        
        return ETSignals(
            learning_progress=np.array(lp_values),
            task_difficulties=np.array(difficulties),
            mdl_complexity=np.random.uniform(0.3, 0.5),  # Modelos cient√≠ficos
            energy_consumption=np.random.uniform(0.2, 0.4),  # Experimentos
            scalability_inverse=np.random.uniform(0.1, 0.3),
            policy_entropy=entropy,
            policy_divergence=divergence,
            drift_penalty=drift,
            curriculum_variance=np.var(difficulties),
            regret_rate=regret,
            embodiment_score=lab_integration,
            phi_components=np.random.uniform(-0.3, 0.3, 4)
        )
    
    def update_performance(self, accepted: bool, score: float) -> float:
        self.hypothesis_count += 1
        
        if accepted and score > 0:
            # Chance de descoberta v√°lida
            if np.random.random() < 0.3:  # 30% chance
                self.validated_discoveries += 1
            
            improvement = min(0.06, score * 0.012)
            self.discovery_rate = min(0.8, self.discovery_rate + improvement)
        else:
            # Degrada√ß√£o na taxa de descoberta
            self.discovery_rate = max(0.1, self.discovery_rate - 0.01)
        
        self.performance_history.append(self.discovery_rate)
        return self.discovery_rate

def run_domain_simulation(simulator: DomainSimulator, 
                         et_params: Dict = None,
                         iterations: int = 500) -> SimulationResults:
    """Executa simula√ß√£o para um dom√≠nio espec√≠fico"""
    
    if et_params is None:
        et_params = {"rho": 1.0, "sigma": 1.0, "iota": 1.0, "gamma": 0.4}
    
    # Ajustar par√¢metros baseado no dom√≠nio
    if simulator.domain_name == "Robotics":
        et_params["iota"] = 2.0  # Embodiment mais importante
    elif simulator.domain_name == "Large Language Model":
        et_params["iota"] = 0.1  # Embodiment menos importante
    
    et = ETCore(**et_params)
    
    scores = []
    decisions = []
    performance_values = []
    guardrails_triggered = 0
    
    current_performance = 0.5  # Performance inicial
    
    logger.info(f"Iniciando simula√ß√£o para {simulator.domain_name}")
    
    for i in range(iterations):
        # Gerar sinais do dom√≠nio
        signals = simulator.generate_signals(i, current_performance)
        
        # Verificar guardrails espec√≠ficos do dom√≠nio
        if simulator.domain_name == "Robotics" and signals.regret_rate > 0.2:
            guardrails_triggered += 1
            logger.warning(f"Guardrail de seguran√ßa ativado na itera√ß√£o {i}")
            continue
        
        # Decis√£o da ET
        accept, score, terms = et.accept_modification(signals)
        
        # Atualizar performance do dom√≠nio
        current_performance = simulator.update_performance(accept, score)
        
        scores.append(score)
        decisions.append(accept)
        performance_values.append(current_performance)
        
        # Log peri√≥dico
        if i % 100 == 0:
            logger.info(f"{simulator.domain_name} - Itera√ß√£o {i}: "
                       f"Performance={current_performance:.3f}, "
                       f"Score={score:.3f}, "
                       f"Aceito={'Sim' if accept else 'N√£o'}")
    
    # Calcular m√©tricas finais
    acceptance_rate = np.mean(decisions)
    stability_metric = np.std(performance_values[-50:])  # Estabilidade final
    final_performance = performance_values[-1]
    
    # Encontrar tempo de converg√™ncia (quando performance estabiliza)
    convergence_time = iterations
    for i in range(50, iterations):
        if np.std(performance_values[i-50:i]) < 0.05:
            convergence_time = i
            break
    
    logger.info(f"Simula√ß√£o {simulator.domain_name} conclu√≠da:")
    logger.info(f"  Taxa de aceita√ß√£o: {acceptance_rate:.2%}")
    logger.info(f"  Performance final: {final_performance:.3f}")
    logger.info(f"  Estabilidade: {stability_metric:.4f}")
    logger.info(f"  Converg√™ncia em: {convergence_time} itera√ß√µes")
    
    return SimulationResults(
        domain=simulator.domain_name,
        scores=scores,
        acceptance_rate=acceptance_rate,
        stability_metric=stability_metric,
        final_performance=final_performance,
        convergence_time=convergence_time,
        guardrails_triggered=guardrails_triggered
    )

def optimize_parameters(simulator: DomainSimulator, 
                       param_ranges: Dict,
                       iterations: int = 200) -> Dict:
    """Otimiza par√¢metros da ET para um dom√≠nio espec√≠fico"""
    
    logger.info(f"Otimizando par√¢metros para {simulator.domain_name}")
    
    best_params = None
    best_score = -np.inf
    
    # Grid search simples
    rho_values = np.linspace(param_ranges["rho"][0], param_ranges["rho"][1], 5)
    sigma_values = np.linspace(param_ranges["sigma"][0], param_ranges["sigma"][1], 5)
    iota_values = np.linspace(param_ranges["iota"][0], param_ranges["iota"][1], 5)
    
    for rho in rho_values:
        for sigma in sigma_values:
            for iota in iota_values:
                params = {"rho": rho, "sigma": sigma, "iota": iota, "gamma": 0.4}
                
                # Executar simula√ß√£o curta
                result = run_domain_simulation(simulator, params, iterations)
                
                # M√©trica de qualidade: performance final + estabilidade
                quality_score = result.final_performance - result.stability_metric
                
                if quality_score > best_score:
                    best_score = quality_score
                    best_params = params.copy()
    
    logger.info(f"Melhores par√¢metros para {simulator.domain_name}: {best_params}")
    logger.info(f"Score de qualidade: {best_score:.4f}")
    
    return best_params

def run_comparative_analysis():
    """Executa an√°lise comparativa entre dom√≠nios"""
    
    print("üî¨ AN√ÅLISE COMPARATIVA ENTRE DOM√çNIOS üî¨\n")
    
    # Criar simuladores
    simulators = [
        RLSimulator(),
        LLMSimulator(),
        RoboticsSimulator(),
        ScientificDiscoverySimulator()
    ]
    
    results = []
    
    # Executar simula√ß√µes
    for simulator in simulators:
        result = run_domain_simulation(simulator, iterations=300)
        results.append(result)
    
    # An√°lise comparativa
    print("\nüìä RESULTADOS COMPARATIVOS:")
    print("=" * 80)
    print(f"{'Dom√≠nio':<25} {'Taxa Aceit.':<12} {'Perf. Final':<12} {'Estabilidade':<12} {'Converg√™ncia':<12}")
    print("=" * 80)
    
    for result in results:
        print(f"{result.domain:<25} {result.acceptance_rate:<12.1%} "
              f"{result.final_performance:<12.3f} {result.stability_metric:<12.4f} "
              f"{result.convergence_time:<12d}")
    
    # Identificar melhor dom√≠nio
    best_domain = max(results, key=lambda r: r.final_performance - r.stability_metric)
    print(f"\nüèÜ Melhor desempenho: {best_domain.domain}")
    
    # An√°lise de guardrails
    total_guardrails = sum(r.guardrails_triggered for r in results)
    print(f"\nüõ°Ô∏è Total de guardrails ativados: {total_guardrails}")
    
    return results

def test_parameter_optimization():
    """Testa otimiza√ß√£o de par√¢metros"""
    
    print("\nüéØ TESTE DE OTIMIZA√á√ÉO DE PAR√ÇMETROS üéØ\n")
    
    # Testar otimiza√ß√£o para RL
    rl_sim = RLSimulator()
    param_ranges = {
        "rho": (0.5, 2.0),
        "sigma": (0.5, 2.0),
        "iota": (0.5, 2.0)
    }
    
    # Par√¢metros padr√£o
    default_params = {"rho": 1.0, "sigma": 1.0, "iota": 1.0, "gamma": 0.4}
    result_default = run_domain_simulation(rl_sim, default_params, 200)
    
    # Par√¢metros otimizados
    rl_sim_opt = RLSimulator()  # Nova inst√¢ncia
    optimized_params = optimize_parameters(rl_sim_opt, param_ranges, 150)
    
    rl_sim_final = RLSimulator()  # Nova inst√¢ncia para teste final
    result_optimized = run_domain_simulation(rl_sim_final, optimized_params, 200)
    
    print(f"\nCompara√ß√£o RL:")
    print(f"Padr√£o:     Performance={result_default.final_performance:.3f}, "
          f"Estabilidade={result_default.stability_metric:.4f}")
    print(f"Otimizado:  Performance={result_optimized.final_performance:.3f}, "
          f"Estabilidade={result_optimized.stability_metric:.4f}")
    
    improvement = result_optimized.final_performance - result_default.final_performance
    print(f"Melhoria: {improvement:.3f} ({improvement/result_default.final_performance:.1%})")

def main():
    """Executa todos os testes pr√°ticos"""
    
    print("üöÄ TESTES PR√ÅTICOS AVAN√áADOS DA EQUA√á√ÉO DE TURING üöÄ\n")
    
    try:
        # An√°lise comparativa entre dom√≠nios
        results = run_comparative_analysis()
        
        # Teste de otimiza√ß√£o de par√¢metros
        test_parameter_optimization()
        
        print("\n‚úÖ TODOS OS TESTES PR√ÅTICOS CONCLU√çDOS COM SUCESSO!")
        
        # Resumo final
        print("\nüìã RESUMO DOS TESTES:")
        print("‚úì Simula√ß√£o de Aprendizado por Refor√ßo")
        print("‚úì Simula√ß√£o de Large Language Models")
        print("‚úì Simula√ß√£o de Rob√≥tica")
        print("‚úì Simula√ß√£o de Descoberta Cient√≠fica")
        print("‚úì An√°lise comparativa entre dom√≠nios")
        print("‚úì Otimiza√ß√£o de par√¢metros")
        print("‚úì Testes de guardrails de seguran√ßa")
        
        # Salvar resultados
        results_summary = {
            "domains": [r.domain for r in results],
            "acceptance_rates": [r.acceptance_rate for r in results],
            "final_performances": [r.final_performance for r in results],
            "stability_metrics": [r.stability_metric for r in results],
            "convergence_times": [r.convergence_time for r in results]
        }
        
        with open("/home/ubuntu/simulation_results.json", "w") as f:
            json.dump(results_summary, f, indent=2)
        
        print(f"\nüíæ Resultados salvos em simulation_results.json")
        
    except Exception as e:
        print(f"‚ùå ERRO NOS TESTES PR√ÅTICOS: {e}")
        raise

if __name__ == "__main__":
    main()

